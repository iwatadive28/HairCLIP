{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxn-I-VVWxs5"
      },
      "source": [
        "Google Colab上でHairCLIPを動作させる環境です。\n",
        "\n",
        "- Respect: [[HairCLIP] 機械学習で顔写真の髪型や髪色を変更する](https://www.12-technology.com/2022/05/hairclip.html)\n",
        "- GitHub: [HairCLIP_demo.ipynb](https://github.com/kaz12tech/ai_demos/blob/main/HairCLIP_demo.ipynb)\n",
        "\n",
        "上記を参考にさせていただきました。基本的には上記のipynbファイルのまま、GoogleDriveマウントとカメラ表示だけ追加しています。\n",
        "\n",
        "HairCLIPの詳細は下記参照。\n",
        "- 論文: https://arxiv.org/abs/2112.05142\n",
        "- GitHub: https://github.com/wty-ustc/HairCLIP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqUeMxwRykyb"
      },
      "source": [
        "# 環境セットアップ\n",
        "## GPU確認"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7qmQjeMW5Ng",
        "outputId": "a13fc29d-d131-4373-f2dd-d1207b36b068"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFioFgRDffBT"
      },
      "source": [
        "# Google Drive マウント"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krUxm6zh_QGU",
        "outputId": "0aa3c3ad-6136-4c50-8577-9f80efa36ce6"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSDkUAykCBn5"
      },
      "source": [
        "1回目だけ、Drive上にフォルダ作成、GitHubからコード取得。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fl1zDy8K_hhZ",
        "outputId": "9f1a3f6b-908b-43e6-dffc-92a2abd6d5f4"
      },
      "outputs": [],
      "source": [
        "!mkdir drive/MyDrive/Colab\\ Notebooks/HairCLIP\n",
        "%cd drive/MyDrive/Colab\\ Notebooks/HairCLIP\n",
        "base_dir = \"/content/drive/MyDrive/Colab Notebooks/HairCLIP\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vl36Yn07WxLK"
      },
      "source": [
        "# GitHubからコード取得"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcdJ6KA3XE3-",
        "outputId": "bc8bd898-672e-49eb-b109-9e2f0e40c185"
      },
      "outputs": [],
      "source": [
        "# %cd /content\n",
        "%cd /content/drive/MyDrive/Colab\\ Notebooks/HairCLIP\n",
        "\n",
        "!git clone https://github.com/wty-ustc/HairCLIP.git\n",
        "!git clone https://github.com/omertov/encoder4editing.git\n",
        "!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
        "!sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
        "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhqOtreBXJCr"
      },
      "source": [
        "# ライブラリのインストール"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnY78_2UXL-q",
        "outputId": "ecfe185a-51b4-4853-c62f-3fa4d547d073"
      },
      "outputs": [],
      "source": [
        "# %cd HairCLIP\n",
        "%cd  /content/drive/MyDrive/Colab\\ Notebooks/HairCLIP/HairCLIP\n",
        "\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install tensorflow-io\n",
        "!pip install --upgrade --no-cache-dir gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XvjEhCjXQ5s"
      },
      "source": [
        "# ライブラリのインポート"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wmk8xvk1XUFa",
        "outputId": "68a49735-e49e-4a7b-a1aa-2575b805d2fd"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/Colab\\ Notebooks/HairCLIP/encoder4editing\n",
        "\n",
        "from utils.alignment import align_face\n",
        "from models.psp import pSp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ig_GYSvZXu_K",
        "outputId": "947f866d-3f7f-443c-8c8c-71d9d162bf36"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/Colab\\ Notebooks/HairCLIP/HairCLIP\n",
        "\n",
        "import os\n",
        "import gdown\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "import sys\n",
        "sys.path.append(\".\")\n",
        "sys.path.append(\"..\")\n",
        "import tempfile\n",
        "from argparse import Namespace\n",
        "\n",
        "import dlib\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "import imageio\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "import glob\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import random\n",
        "\n",
        "from criteria.parse_related_loss import average_lab_color_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lg-QeYgtFoMV",
        "outputId": "33ebc921-765b-482f-a1da-1c81803f88f1"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "try:\n",
        "  filename = take_photo()\n",
        "  print('Saved to {}'.format(filename))\n",
        "  \n",
        "  # Show the image which was just taken.\n",
        "  display(Image(filename))\n",
        "except Exception as err:\n",
        "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "  # grant the page permission to access it.\n",
        "  print(str(err))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Sj5XU1rY4-g",
        "outputId": "a2aa8285-95f0-40f4-d1a5-efb30b1762c4"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/Colab\\ Notebooks/HairCLIP/HairCLIP/mapper\n",
        "\n",
        "from mapper.datasets.latents_dataset_inference import LatentsDatasetInference\n",
        "from mapper.hairclip_mapper import HairCLIPMapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baAOvq6tY7Ne"
      },
      "source": [
        "# 学習済みモデルのダウンロード\n",
        "Access denied with the following error:\n",
        "が発生する場合、何回か実行"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZ0BW55sY-G4",
        "outputId": "050ada96-d265-4ae2-d4c6-9d89a6711b33"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/Colab\\ Notebooks/HairCLIP\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "download_with_pydrive = True\n",
        "\n",
        "class Downloader(object):\n",
        "    def __init__(self, use_pydrive):\n",
        "        self.use_pydrive = use_pydrive\n",
        "        current_directory = os.getcwd()\n",
        "        \n",
        "        self.save_dir = base_dir + \"/HairCLIP/pretrained_models\"\n",
        "        os.makedirs(self.save_dir, exist_ok=True)\n",
        "        if self.use_pydrive:\n",
        "            self.authenticate()\n",
        "\n",
        "    def authenticate(self):\n",
        "        auth.authenticate_user()\n",
        "        gauth = GoogleAuth()\n",
        "        gauth.credentials = GoogleCredentials.get_application_default()\n",
        "        self.drive = GoogleDrive(gauth)\n",
        "\n",
        "    def download_file(self, file_id, file_name):\n",
        "        file_dst = f'{self.save_dir}/{file_name}'\n",
        "        if os.path.exists(file_dst):\n",
        "            print(f'{file_dst} already exists!')\n",
        "            return\n",
        "        if self.use_pydrive:\n",
        "            downloaded = self.drive.CreateFile({'id':file_id})\n",
        "            downloaded.FetchMetadata(fetch_all=True)\n",
        "            downloaded.GetContentFile(file_dst)\n",
        "        else:\n",
        "            !gdown --id $file_id -O $file_dst\n",
        "\n",
        "downloader = Downloader(download_with_pydrive)\n",
        "downloader.download_file(file_id=\"1cUv_reLE6k3604or78EranS7XzuVMWeO\", file_name=\"e4e_ffhq_encode.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxQou1JaZdRp",
        "outputId": "27e25a50-9a74-41ba-ed86-da5af0da595f"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/Colab\\ Notebooks/HairCLIP/HairCLIP\n",
        "\n",
        "if not os.path.exists(\"./pretrained_models/hairclip.pt\"):\n",
        "  gdown.download('https://drive.google.com/uc?id=1hqZT6ZMldhX3M_x378Sm4Z2HMYr-UwQ4', \"./pretrained_models/hairclip.pt\", quiet=False)\n",
        "if not os.path.exists(\"./pretrained_models/stylegan2-ffhq-config-f.pt\"):\n",
        "  gdown.download('https://drive.google.com/uc?id=1pts5tkfAcWrg4TpLDu6ILF5wHID32Nzm', \"./pretrained_models/stylegan2-ffhq-config-f.pt\", quiet=False)\n",
        "if not os.path.exists(\"./pretrained_models/model_ir_se50.pth\"):\n",
        "  gdown.download('https://drive.google.com/uc?id=1FS2V756j-4kWduGxfir55cMni5mZvBTv', \"./pretrained_models/model_ir_se50.pth\", quiet=False)\n",
        "\n",
        "\n",
        "# if not os.path.exists(\"./pretrained_models/test_faces.pt\"):\n",
        "#   gdown.download('https://drive.google.com/uc?id=1j7RIfmrCoisxx3t-r-KC02Qc8barBecr', \"./pretrained_models/test_faces.pt\", quiet=False)\n",
        "\n",
        "if not os.path.exists(\"./pretrained_models/shape_predictor_68_face_landmarks.dat.bz2\"):\n",
        "  !wget -c http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2 \\\n",
        "        -O ./pretrained_models/shape_predictor_68_face_landmarks.dat.bz2\n",
        "  !bzip2 -dk ./pretrained_models/shape_predictor_68_face_landmarks.dat.bz2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZhIWcowZg1I"
      },
      "source": [
        "# テスト画像のセットアップ\n",
        "## テスト画像のダウンロード\n",
        "[サンプル画像](https://www.pakutaso.com/20190117010post-18492.html)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWthIMOyZsMj",
        "outputId": "29135176-7940-45ba-8be6-4875caf1a135"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/Colab Notebooks/HairCLIP/HairCLIP\n",
        "!mkdir demo\n",
        "\n",
        "!wget -c https://www.pakutaso.com/shared/img/thumb/model10211041_TP_V4.jpg \\\n",
        "      -O ./demo/model10211041_TP_V4.jpg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbIfsG_eZuo-"
      },
      "source": [
        "# Edit Hair\n",
        "## Set parametor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_rAbvIAEZy0s",
        "outputId": "555b2be6-7013-4c51-a1c8-84d98b9f3870"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/Colab Notebooks/HairCLIP/HairCLIP\n",
        "\n",
        "# @markdown 入力画像\n",
        "# image_path = base_dir + \"/HairCLIP/demo/model10211041_TP_V4.jpg\" #@param {type:\"string\"}\n",
        "image_path = base_dir + \"/HairCLIP/demo/face_demo.JPG\" #@param {type:\"string\"}\n",
        "\n",
        "# @markdown editing_type=\"both\"でrandomに生成<br>\n",
        "# @markdown randomの場合下記の設定は反映されません。\n",
        "IsRandom = False #@param {type:\"boolean\"}\n",
        "random_num = 30 #@param {type:\"integer\"}\n",
        "\n",
        "# @markdown randomではない場合は以下設定<br>\n",
        "# @markdown 編集タイプ colorのみ、styleのみ、両方\n",
        "editing_type = \"both\" #@param[\"hairstyle\", \"color\", \"both\"]\n",
        "# @markdown 髪型選択\n",
        "hairstyle_description = \"crew cut hairstyle\" #@param[\"afro hairstyle\", \"bob cut hairstyle\", \"bowl cut hairstyle\", \"braid hairstyle\", \"caesar cut hairstyle\", \"chignon hairstyle\", \"cornrows hairstyle\", \"crew cut hairstyle\", \"crown braid hairstyle\", \"curtained hair hairstyle\", \"dido flip hairstyle\", \"dreadlocks hairstyle\", \"extensions hairstyle\", \"fade hairstyle\", \"fauxhawk hairstyle\", \"finger waves hairstyle\", \"french braid hairstyle\", \"frosted tips hairstyle\", \"full crown hairstyle\", \"harvard clip hairstyle\", \"high and tight hairstyle\", \"hime cut hairstyle\", \"hi-top fade hairstyle\",\"jewfro hairstyle\", \"jheri curl hairstyle\", \"liberty spikes hairstyle\", \"marcel waves hairstyle\", \"mohawk hairstyle\", \"pageboy hairstyle\", \"perm hairstyle\", \"pixie cut hairstyle\", \"psychobilly wedge hairstyle\", \"quiff hairstyle\", \"regular taper cut hairstyle\", \"ringlets hairstyle\", \"shingle bob hairstyle\", \"short hair hairstyle\", \"slicked-back hairstyle\", \"spiky hair hairstyle\",\"surfer hair hairstyle\", \"taper cut hairstyle\", \"the rachel hairstyle\", \"undercut hairstyle\", \"updo hairstyle\"]\n",
        "# @markdown 髪色選択\n",
        "color_description = \"yellow\" #@param[\"purple\", \"red\", \"orange\", \"yellow\", \"green\", \"blue\", \"gray\", \"brown\", \"black\", \"white\", \"blond\", \"pink\"]\n",
        "\n",
        "# 出力先ディレクトリ作成\n",
        "!mkdir outputs\n",
        "\n",
        "from IPython.display import Image,display_jpeg\n",
        "display_jpeg(Image(image_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jms-bvJ2Z486"
      },
      "source": [
        "# Define functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-Eq9eOGZ6iM"
      },
      "outputs": [],
      "source": [
        "def run_alignment(image_path):\n",
        "  predictor = dlib.shape_predictor(base_dir + \"/HairCLIP/pretrained_models/shape_predictor_68_face_landmarks.dat\")\n",
        "  aligned_image = align_face(filepath=image_path, predictor=predictor)\n",
        "  print(\"Aligned image has shape: {}\".format(aligned_image.size))\n",
        "  return aligned_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rvwt9ARIFd7D"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B04OTWW_Z8tX"
      },
      "outputs": [],
      "source": [
        "def run_on_batch_e4e(inputs, net):\n",
        "  images, latents = net(\n",
        "      inputs.to(\"cuda\").float(), randomize_noise=False, return_latents=True\n",
        "      )\n",
        "  return images, latents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XM1PDDAdafkq"
      },
      "outputs": [],
      "source": [
        "def run_on_batch(\n",
        "    inputs,\n",
        "    hairstyle_text_inputs,\n",
        "    color_text_inputs,\n",
        "    hairstyle_tensor_hairmasked,\n",
        "    color_tensor_hairmasked,\n",
        "    net,\n",
        "):\n",
        "    w = inputs\n",
        "    with torch.no_grad():\n",
        "        w_hat = w + 0.1 * net.mapper(\n",
        "            w,\n",
        "            hairstyle_text_inputs,\n",
        "            color_text_inputs,\n",
        "            hairstyle_tensor_hairmasked,\n",
        "            color_tensor_hairmasked,\n",
        "        )\n",
        "        x_hat, w_hat = net.decoder(\n",
        "            [w_hat],\n",
        "            input_is_latent=True,\n",
        "            return_latents=True,\n",
        "            randomize_noise=False,\n",
        "            truncation=1,\n",
        "        )\n",
        "        x, _ = net.decoder(\n",
        "            [w], input_is_latent=True, randomize_noise=False, truncation=1\n",
        "        )\n",
        "        result_batch = (x_hat, w_hat, x)\n",
        "    return result_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba8s5AH7aipD"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image,display_png\n",
        "def predict(\n",
        "    edit_t, hair_d, color_d, \n",
        "    ck, im_path, trans, device):\n",
        "    editing_type_ = edit_t\n",
        "    hairstyle_description_ = hair_d\n",
        "    color_description_ = color_d\n",
        "\n",
        "    if editing_type_ == \"both\":\n",
        "      assert (\n",
        "          hairstyle_description_ is not None and color_d is not None\n",
        "          ), (\"Please provide description \" \"for both hairstyle and color.\")\n",
        "    elif editing_type_ == \"hairstyle\":\n",
        "      assert (\n",
        "          hairstyle_description_ is not None\n",
        "          ), \"Please provide description for hairstyle.\"\n",
        "    else:\n",
        "      assert (\n",
        "          color_description_ is not None\n",
        "          ), \"Please provide description for color.\"\n",
        "\n",
        "    opts = ck[\"opts\"]\n",
        "    opts = Namespace(**opts)\n",
        "    opts.editing_type = editing_type_\n",
        "    opts.input_type = \"text\"\n",
        "    opts.color_description = color_description_\n",
        "\n",
        "    if hair_d is not None:\n",
        "      with open(base_dir + \"/HairCLIP/outputs/hairstyle_description.txt\", \"w\") as file:\n",
        "        file.write(hairstyle_description_)\n",
        "      opts.hairstyle_description = base_dir + \"/HairCLIP/outputs/hairstyle_description.txt\"\n",
        "\n",
        "    opts.checkpoint_path  = base_dir + \"/HairCLIP/pretrained_models/hairclip.pt\"\n",
        "    opts.parsenet_weights = base_dir + \"/HairCLIP/pretrained_models/parsenet.pth\"\n",
        "    opts.stylegan_weights = base_dir + \"/HairCLIP/pretrained_models/stylegan2-ffhq-config-f.pt\"\n",
        "    opts.ir_se50_weights  = base_dir + \"/HairCLIP/pretrained_models/model_ir_se50.pth\"\n",
        "    net = HairCLIPMapper(opts)\n",
        "    net.eval()\n",
        "    net.cuda()\n",
        "\n",
        "    # 顔部分のalignment, transform\n",
        "    input_image = run_alignment(str(im_path))\n",
        "    resize_dims = (256, 256)\n",
        "    input_image.resize(resize_dims)\n",
        "    transformed_image = trans(input_image)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      images, latents = run_on_batch_e4e(\n",
        "          transformed_image.unsqueeze(0), e4e_net\n",
        "          )\n",
        "      print(\"Latent code calculated!\")\n",
        "\n",
        "    dataset = LatentsDatasetInference(latents=latents.cpu(), opts=opts)\n",
        "    dataloader = DataLoader(dataset)\n",
        "\n",
        "    average_color_loss = (\n",
        "        average_lab_color_loss.AvgLabLoss(opts).to(device).eval()\n",
        "        )\n",
        "\n",
        "    out_filename = editing_type_ + \"_\" \\\n",
        "      + hairstyle_description_ + \"_\" \\\n",
        "      + color_description_ + \".png\"\n",
        "    out_path = os.path.join(base_dir + \"/HairCLIP/outputs\", out_filename)\n",
        "    print(\"output path:\", out_path)\n",
        "\n",
        "    for input_batch in tqdm(dataloader):\n",
        "      with torch.no_grad():\n",
        "        w,hairstyle_text_inputs_list, color_text_inputs_list, selected_description_tuple_list, hairstyle_tensor_list, color_tensor_list = input_batch\n",
        "\n",
        "        hairstyle_text_inputs = hairstyle_text_inputs_list[0]\n",
        "        color_text_inputs = color_text_inputs_list[0]\n",
        "\n",
        "        selected_description = selected_description_tuple_list[0][0]\n",
        "        hairstyle_tensor = hairstyle_tensor_list[0]\n",
        "        color_tensor = color_tensor_list[0]\n",
        "\n",
        "        w = w.cuda().float()\n",
        "        hairstyle_text_inputs = hairstyle_text_inputs.cuda()\n",
        "        color_text_inputs = color_text_inputs.cuda()\n",
        "        hairstyle_tensor = hairstyle_tensor.cuda()\n",
        "        color_tensor = color_tensor.cuda()\n",
        "        if hairstyle_tensor.shape[1] != 1:\n",
        "          hairstyle_tensor_hairmasked = (\n",
        "              hairstyle_tensor * average_color_loss.gen_hair_mask(hairstyle_tensor))\n",
        "        else:\n",
        "          hairstyle_tensor_hairmasked = torch.Tensor([0]).unsqueeze(0).cuda()\n",
        "\n",
        "        if color_tensor.shape[1] != 1:\n",
        "          color_tensor_hairmasked = (\n",
        "              color_tensor * average_color_loss.gen_hair_mask(color_tensor))\n",
        "        else:\n",
        "          color_tensor_hairmasked = torch.Tensor([0]).unsqueeze(0).cuda()\n",
        "          result_batch = run_on_batch(\n",
        "              w, hairstyle_text_inputs, color_text_inputs,\n",
        "              hairstyle_tensor_hairmasked, color_tensor_hairmasked, net,)   \n",
        "\n",
        "        if (hairstyle_tensor.shape[1] != 1) and (color_tensor.shape[1] != 1):\n",
        "          img_tensor = torch.cat([hairstyle_tensor, color_tensor], dim=3)\n",
        "        elif hairstyle_tensor.shape[1] != 1:\n",
        "          img_tensor = hairstyle_tensor\n",
        "        elif color_tensor.shape[1] != 1:\n",
        "          img_tensor = color_tensor\n",
        "        else:\n",
        "          img_tensor = None   \n",
        "\n",
        "        if img_tensor is not None:\n",
        "          if img_tensor.shape[3] == 1024:\n",
        "            couple_output = torch.cat(\n",
        "                [result_batch[2][0].unsqueeze(0), result_batch[0][0].unsqueeze(0), img_tensor,])\n",
        "          elif img_tensor.shape[3] == 2048:\n",
        "            couple_output = torch.cat(\n",
        "                [result_batch[2][0].unsqueeze(0), result_batch[0][0].unsqueeze(0),\n",
        "                 img_tensor[:, :, :, 0:1024], img_tensor[:, :, :, 1024::], ])\n",
        "            couple_output = torch.cat(\n",
        "                [result_batch[2][0].unsqueeze(0), result_batch[0][0].unsqueeze(0),\n",
        "                 img_tensor[:, :, :, 0:1024], img_tensor[:, :, :, 1024::], ])\n",
        "        else:\n",
        "            couple_output = torch.cat(\n",
        "                [result_batch[2][0].unsqueeze(0),result_batch[0][0].unsqueeze(0),])\n",
        "\n",
        "        torchvision.utils.save_image(\n",
        "            couple_output, str(out_path), normalize=True, range=(-1, 1))\n",
        "        \n",
        "        \n",
        "        display_png(Image(out_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fX3WAqgakdP"
      },
      "source": [
        "# Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7LjnnMNPtd1"
      },
      "outputs": [],
      "source": [
        "base_dir = \"/content/drive/MyDrive/Colab Notebooks/HairCLIP\"\n",
        "with open(base_dir + \"/HairCLIP/mapper/hairstyle_list.txt\") as infile:\n",
        "  HAIRSTYLE_LIST = sorted([line.rstrip() for line in infile])\n",
        "COLORSTYLE_LIST = [\"purple\", \"red\", \"orange\", \"yellow\", \"green\", \"blue\", \"gray\", \"brown\", \"black\", \"white\", \"blond\", \"pink\"]\n",
        "\n",
        "device = \"cuda:0\"\n",
        "\n",
        "# load e4e ffhq model\n",
        "e4e_model_path = base_dir + \"/HairCLIP/pretrained_models/e4e_ffhq_encode.pt\"\n",
        "e4e_ckpt = torch.load(e4e_model_path, map_location=\"cpu\")\n",
        "e4e_opts = e4e_ckpt[\"opts\"]\n",
        "e4e_opts[\"checkpoint_path\"] = e4e_model_path\n",
        "e4e_opts = Namespace(**e4e_opts)\n",
        "\n",
        "e4e_net = pSp(e4e_opts)\n",
        "e4e_net.eval()\n",
        "e4e_net.cuda()\n",
        "print(\"e4e model successfully loaded!\")\n",
        "\n",
        "# set transforms\n",
        "img_transforms = transforms.Compose(\n",
        "    [\n",
        "     transforms.Resize((256, 256)),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
        "    ])\n",
        "\n",
        "# load hairclip model\n",
        "checkpoint_path = base_dir + \"/HairCLIP/pretrained_models/hairclip.pt\"\n",
        "ckpt = torch.load(checkpoint_path, map_location=\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81vbHt_vKOwp"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "id": "8bYxdLSIKOww",
        "outputId": "e58fb7c4-93cd-4e62-d88e-78dc37ee7ac5"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/Colab Notebooks/HairCLIP/HairCLIP\n",
        "\n",
        "from IPython.display import Image\n",
        "try:\n",
        "  name = 'demo/photo.jpg'\n",
        "  filename = take_photo(name)\n",
        "  print('Saved to {}'.format(filename))\n",
        "  \n",
        "  # Show the image which was just taken.\n",
        "  display(Image(filename))\n",
        "  \n",
        "except Exception as err:\n",
        "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "  # grant the page permission to access it.\n",
        "  print(str(err))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "qlJTkYQSMdKR",
        "outputId": "dcbb32bd-de38-4acc-e6b2-58e4693b9862"
      },
      "outputs": [],
      "source": [
        "# @markdown 入力画像\n",
        "# image_path = base_dir + \"/HairCLIP/demo/model10211041_TP_V4.jpg\" #@param {type:\"string\"}\n",
        "image_path = base_dir + \"/HairCLIP/demo/photo.jpg\" #@param {type:\"string\"}\n",
        "\n",
        "from IPython.display import Image,display_jpeg\n",
        "display_jpeg(Image(image_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "g4IJCQMNamBr",
        "outputId": "30ba41d4-78da-4c2d-924c-f03e792af352"
      },
      "outputs": [],
      "source": [
        "if IsRandom == True:\n",
        "  for i in range(random_num):\n",
        "    hairstyle_index = random.randrange(0, (len(HAIRSTYLE_LIST)-1), 1)\n",
        "    colorstyle_index = random.randrange(0, (len(COLORSTYLE_LIST)-1), 1)\n",
        "    predict(\n",
        "        \"both\", HAIRSTYLE_LIST[hairstyle_index], COLORSTYLE_LIST[colorstyle_index], \n",
        "        ckpt, image_path, img_transforms, device)\n",
        "\n",
        "else:\n",
        "  predict(\n",
        "      editing_type, hairstyle_description, color_description, \n",
        "      ckpt, image_path, img_transforms, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7jCqdrkatPX"
      },
      "source": [
        "\n",
        "# Show Result\n",
        "\n",
        "mp4動画を作成して再生するところまで。必要あれば実行。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rg1FuKELcNpT",
        "outputId": "6e5ab8bd-fd0b-4868-dceb-ff6bb644db2f"
      },
      "outputs": [],
      "source": [
        "# !pip install imageio-ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "lhctxlZ7auy5",
        "outputId": "1a16787a-eb1c-42be-8d45-2451290d54b8"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "def generate_mp4(out_name, images, kwargs):\n",
        "  print(out_name + '.mp4')\n",
        "  writer = imageio.get_writer(out_name + '.mp4', **kwargs)\n",
        "  for image in images:\n",
        "    writer.append_data(image)\n",
        "  writer.close()\n",
        "\n",
        "def show_mp4(filename, width):\n",
        "  print(filename+'.mp4')\n",
        "  mp4 = open(filename + '.mp4', 'rb').read()\n",
        "  data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "  display(HTML(\"\"\"\n",
        "  <video width=\"%d\" controls autoplay loop>\n",
        "    <source src=\"%s\" type=\"video/mp4\">\n",
        "  </video>\n",
        "  \"\"\" % (width, data_url)))\n",
        "\n",
        "res_list = glob.glob(base_dir + \"/HairCLIP/outputs/*.png\")\n",
        "images = []\n",
        "for img_path in res_list:\n",
        "  images.append(np.array(Image.open(img_path)))\n",
        "\n",
        "kwargs = {'fps': 2}\n",
        "\n",
        "gif_path = os.path.join(base_dir + \"/HairCLIP/outputs\", \"animation\")\n",
        "generate_mp4(gif_path, images, kwargs)\n",
        "show_mp4(gif_path, width=514)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hisJMbuKFfM3"
      },
      "source": [
        "# GradCAM\n",
        "ここからテスト、うまく作業中。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKXURsX3FxCE",
        "outputId": "f431be8f-07bd-4588-ce58-9b358aea8fd5"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "!pip install pytorch-gradcam\n",
        "!pip install grad-cam\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707,
          "referenced_widgets": [
            "0de2862d51b74abab995e4cd197b5fc9",
            "630bde66fa4743fcb8783cd49d220536",
            "e5bf15e593d14b65803b768bbf7cbf0e",
            "c75918d421a443f482fe932d844e50c8",
            "455933a39eac43ed828c7eba3b3cf43a",
            "86aa5221ff214e148685105376674c42",
            "ab9b0aae994d4efcb351a3ca48295660",
            "86ded3295fdc45d9aab0bed0e2371290",
            "3527bf80f705412ba5be42ac1f8e6681",
            "16cd35afc48a4031831c1fe83db93169",
            "d0b50216b5c14e1195f19160f79a4a51"
          ]
        },
        "id": "GwTBFtdDFhqc",
        "outputId": "d5360a54-7e2f-417c-d93b-7537ee0175c4"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# Basic Modules\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# PyTorch Modules\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.dataset import Subset\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "from torchvision.utils import make_grid, save_image\n",
        "\n",
        "# Grad-CAM\n",
        "from gradcam.utils import visualize_cam\n",
        "from gradcam import GradCAM, GradCAMpp\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available()  else \"cpu\")\n",
        "model = models.densenet161(pretrained=True)\n",
        "model.fc = nn.Linear(2048,5)\n",
        "model = torch.nn.DataParallel(model).to(device)\n",
        "model.eval()\n",
        "#  model.load_state_dict(torch.load('trained_model.pt'))\n",
        "# model.load_state_dict(torch.load('/content/HairCLIP/pretrained_models/e4e_ffhq_encode.pt'))\n",
        "model.load_state_dict(torch.load(base_dir + '/HairCLIP/pretrained_models/hairclip.pt'))\n",
        "\n",
        "# Grad-CAM\n",
        "target_layer = model.module.features\n",
        "gradcam = GradCAM(model, target_layer)\n",
        "gradcam_pp = GradCAMpp(model, target_layer)\n",
        "\n",
        "images = []\n",
        "# あるラベルの検証用データセットを呼び出してる想定\n",
        "for path in glob.glob(\"{}/label1/*\".format(config['dataset'])):\n",
        "    img = Image.open(path)\n",
        "    torch_img = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor()\n",
        "    ])(img).to(device)\n",
        "    normed_torch_img = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(torch_img)[None]\n",
        "    mask, _ = gradcam(normed_torch_img)\n",
        "    heatmap, result = visualize_cam(mask, torch_img)\n",
        "\n",
        "    mask_pp, _ = gradcam_pp(normed_torch_img)\n",
        "    heatmap_pp, result_pp = visualize_cam(mask_pp, torch_img)\n",
        "\n",
        "    images.extend([torch_img.cpu(), heatmap, heatmap_pp, result, result_pp])\n",
        "grid_image = make_grid(images, nrow=5)\n",
        "\n",
        "# 結果の表示\n",
        "transforms.ToPILImage()(grid_image)\n",
        "\"\"\"\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "HairCLIP_demo.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "caac486ee1b4bdfec8bec0bf8e440f50717bcb3f3366accd2e3c48bf119de293"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0de2862d51b74abab995e4cd197b5fc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_630bde66fa4743fcb8783cd49d220536",
              "IPY_MODEL_e5bf15e593d14b65803b768bbf7cbf0e",
              "IPY_MODEL_c75918d421a443f482fe932d844e50c8"
            ],
            "layout": "IPY_MODEL_455933a39eac43ed828c7eba3b3cf43a"
          }
        },
        "16cd35afc48a4031831c1fe83db93169": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3527bf80f705412ba5be42ac1f8e6681": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "455933a39eac43ed828c7eba3b3cf43a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "630bde66fa4743fcb8783cd49d220536": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86aa5221ff214e148685105376674c42",
            "placeholder": "​",
            "style": "IPY_MODEL_ab9b0aae994d4efcb351a3ca48295660",
            "value": "100%"
          }
        },
        "86aa5221ff214e148685105376674c42": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86ded3295fdc45d9aab0bed0e2371290": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab9b0aae994d4efcb351a3ca48295660": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c75918d421a443f482fe932d844e50c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16cd35afc48a4031831c1fe83db93169",
            "placeholder": "​",
            "style": "IPY_MODEL_d0b50216b5c14e1195f19160f79a4a51",
            "value": " 110M/110M [00:01&lt;00:00, 94.5MB/s]"
          }
        },
        "d0b50216b5c14e1195f19160f79a4a51": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5bf15e593d14b65803b768bbf7cbf0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86ded3295fdc45d9aab0bed0e2371290",
            "max": 115730790,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3527bf80f705412ba5be42ac1f8e6681",
            "value": 115730790
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
